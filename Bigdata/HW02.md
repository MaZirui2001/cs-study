# HW02

## EX1

### Q1

注意到$\bold v_1$是最大奇异值$\sigma_1$的奇异向量，故有:
$$
|\bold v_1|=1
\\ A\bold v_1 = \sigma_1\bold u_1
$$
可得：
$$
|\bold u_1^TA| = |\bold u_1^T\sum_{i=1}^r\sigma_i\bold u_i \bold v_i^T|= |\bold u_1^T\sigma_1\bold u_1\bold v_1^T| = \sigma_1|\bold v_1^T| = \sigma_1
$$

### Q2

设$\bold u$为单位向量，将其写为：
$$
\bold u = \sum_{i=1}^r\alpha_i\bold u_i
$$
则有：
$$
\sum_{i=1}^r\alpha_i^2=1
$$


带入计算：
$$
\begin{align}
||\bold u^TA|| &= ||\sum_{i=1}^r\alpha_i\bold u_i\cdot \sum_{i=1}^r\sigma_i\bold u_i\bold v_i||
\\&=||\sum_{i=1}^r\alpha_i\sigma_i\bold{v_i}||
\\& \leq \sigma_1||\sum_{i=1}^r\alpha_i\bold v_i||
\\&=\sigma_1\sqrt{\sum_{i=1}^r\alpha_i^2} 
\\ &= \sigma_1
\end{align}
$$
再由$||\bold u^TA|| \leq \sigma_1$，可得：
$$
||\bold u_1^TA|| = \sigma_1=max_{||\bold u||=1}||\bold u^TA||
$$


## EX2

令$A = \sum_{i=1}^r \sigma_i\bold u_i\bold v_i^T=\sum_{i=1}^d \sigma_i\bold u_i\bold v_i^T$，其中$\sigma_{i+1}=...=\sigma_d=0$。

将x改写为：
$$
\bold x = \sum_{i=1}^d\alpha_i\bold v_i
$$
 再令$B=A^TA$，则有：
$$
B^k\bold x=(A^TA)^k\bold x=\sum_{i=1}^d\sigma_i^{2k}\alpha_i\bold v_i
$$
并注意到以下事实：
$$
|\alpha_i|=|x^T\bold v_i| \geq \delta
$$
由$\sigma_2 < \frac12\sigma_1$：
$$
\begin{align}
||B^k\bold x||^2 
&= ||\sum_{i=1}^d\sigma_i^{2k}\alpha_i\bold v_i||^2
\\&=\sum_{i=1}^d\sigma_i^{4k}\alpha_i^2 
\\&\leq \sigma_1^{4k}\alpha_1^2+\sigma_2^{4k}(1-\alpha_1^2) 
\\&< \sigma_1^{4k}[(\frac12)^{4k}(1-\alpha_1^2)+\alpha_1^2]
\end{align}
$$
结合$k = -\log_4{\epsilon\delta}$：
$$
\begin{align}
|\bold w^T\bold v_1|\

&>\frac{(B^kx)^T\bold v_1}{\sqrt{\sigma_1^{4k}((\frac12)^{4k}(1-\alpha_1^2)+\alpha_1^2)}}

\\&=\frac{\sum_{i=1}^d\sigma_i^{2k}\alpha_i\bold v_i^T\cdot \bold v_1}{\sqrt{\sigma_1^{4k}((\frac12)^{4k}(1-\alpha_1^2)+\alpha_1^2)}} 

\\&= \frac{\sigma_1^{2k}\alpha_1}{\sqrt{\sigma_1^{4k}((\frac12)^{4k}(1-\alpha_1^2)+\alpha_1^2)}}

\\&= \frac{1}{\sqrt{(\frac12)^{4k}(\frac1{\alpha_1^2}-1)+1}}

\\&= \frac{1}{\sqrt{(\frac12)^{-2\log_2\epsilon\delta}(\frac1{\alpha_1^2}-1)+1}}

\\&= \frac{1}{\sqrt{{(\epsilon\delta)^{2}}(\frac1{\alpha_1^2}-1)+1}}

\\&\geq \frac{1}{\sqrt{{(\epsilon\delta)^{2}}(\frac1{\delta^2}-1)+1}}

\\&= \frac{1}{\sqrt{\epsilon^{2}(1- \delta^2)+1}}

\\&> \frac{1}{\sqrt{\epsilon^{2}+1}}

\\& \geq 1-\frac12\epsilon^2（伯努利不等式）
\end{align}
$$
当$\epsilon \in[0, 2]$时，$\frac12\epsilon^2 \leq \epsilon$，即$1-\frac12\epsilon^2 \geq 1-\epsilon$，题目得证。由题意可知，$\epsilon$恒为整数；再可知当$\epsilon > 2$时，$1-\epsilon$恒为负数，而模长显然恒为正数，题目显然得证。

综上所述，题目得证。



## EX3

### Q1

根据题意，$E[u_{ij}] = \frac12 \times (-1) + \frac12 \times 1 = 0$, $a$ 与$u_{ij}$独立

我们有：
$$
\begin {align}
E[b_j]&=E[\frac1{\sqrt k}\sum_{i=1}^da_iu_{ij}] \\
&= \frac1{\sqrt k}\sum_{i=1}^da_iE[u_{ij}] \\ 
&= \frac1{\sqrt k}\sum_{i=1}^da_i \cdot0 \\
&= 0
\end{align}
$$


### Q2

根据题意中有关$u_{ij}$的独立性，显然有:
$$
E[u_{ij}^2] = 1 \\
E[u_{ij}u_{lj}] = 0
$$
我们按照下面第二部分的方法来分解这个期望：
$$
\begin{align*}
E[b_j^2] &= E[(\frac1{\sqrt k}\sum_{i=1}^da_iu_{ij})^2] \\
&= \frac1k(\sum_{i=1}^dE[a_i^2u_{ij}^2]+\sum_{i \ne l}E[a_ia_lu_{ij}u_{lj}])\\
&= \frac1k(\sum_{i=1}^dE[a_i^2]E[u_{ij}^2]+\sum_{i \ne l}E[a_ia_l]E[u_{ij}u_{lj}])\\
&= \frac1k(\sum_{i=1}^dE[a_i^2]+\sum_{i \ne l}E[a_ia_l]\cdot 0)\\
&= \frac1k\sum_{i=1}^dE[a_i^2]\\
&= \frac1kE[\sum_{i=1}^da_i^2]\\
&= \frac1k||a||_2^2
\end{align*}
$$


### Q3

根据题意，我们有：
$$
\begin{align}
E[||f(a)||^2] &= E[\sum_{j=1}^k(\frac1{\sqrt k}\sum_{i=1}^da_iu_{ij})^2)] \\
&=\sum_{j=1}^kE[b_j^2]\\
&= \frac1k\sum_{j=1}^k||a||_2^2 \\
&= ||a||_2^2
\end{align}
$$

## EX4

设$k \in \mathbb N$，满足
$$
k = \lceil\log_{0.4}(\delta)\rceil
$$
将$\mathcal A$作为子过程$k$次，来提高成功率。

下面介绍$\mathcal B$的算法描述：

> 1	for i = 1 to k *begin*
>
> 2		Query $\mathcal A$ on the input vertex $x$
>
> 3		if $\mathcal A$ output some $a_i \in \mathcal P$ with $d(x, a_i) \leq c \cdot r$ *begin*
>
> 4 			output $a_i$
>
> 5 			halt()
>
> 6		*end*
>
> 7 	*end*
>
> 8 	// If none of the $k$ iterations output a point $a_i\in \mathcal{P}$ with $d(x,a_i)\leq c\cdot r$,
>
> 9 	output a point $a_{k+1}$ in $\mathcal P$ randomly 
>
> 10	halt()

正确性和成功概率：$\mathcal B$未能输出$d(x, a_i) \le c \cdot r$的点的概率就是$k$次迭代都未能输出这样的点的概率，就是$0.4^k$，根据选择的$k$，有$0.4^k \leq \delta$，故成功概率至少为$1-\delta$

查询时间：查询时间是$\mathcal A$的$k$倍，因此时间为$kT_\mathcal A= \lceil\log_{0.4}(\delta)\rceil T_\mathcal A$



## EX5

### Q1

先证明：
$$
E[\frac{(1+\alpha)^{X_n}}\alpha]=n+\frac1\alpha
$$
由归纳法证明：

当$n=0$时，结论显然成立

当$n > 0$时：
$$
\begin{align}
E[\frac{(1+\alpha)^{X_{n+1}}}\alpha]&=\sum_{j=0}^\infty P(X_n=j)E[\frac{(1+\alpha)^{X_{n+1}}}\alpha|X_n=j] \\
&=\sum_{j=0}^\infty P(X_n=j)((1-\frac1{(1+\alpha)^j})\cdot \frac{(1+\alpha)^j}\alpha + \frac1{(1+\alpha)^j}\cdot\frac{(1+\alpha)^{j+1}}\alpha) \\
&=\sum_{j=0}^\infty P(X_n=j)(\frac{(1+\alpha)^j}\alpha - \frac1\alpha + \frac{1+\alpha}\alpha) \\
&=\sum_{j=0}^\infty P(X_n=j)(\frac{(1+\alpha)^j}\alpha + 1) \\
&=\sum_{j=0}^\infty P(X_n=j)\frac{(1+\alpha)^j}\alpha + 1 \\
&=E[\frac{(1+\alpha)^{X_n}}\alpha] + 1 \\
&= (n+1) + \frac1\alpha
\end{align}
$$
由归纳法，得证

因此：
$$
E[\frac{(1+\alpha)^{X_n}-1}\alpha]=n+\frac1\alpha-\frac1\alpha=n
$$
由此可得：
$$
E[(1+\alpha)^{X_n}]=\alpha n+1
$$
接下来求方差，显然有：
$$
Var[\frac{(1+\alpha)^{X_n}}\alpha]=\frac1{\alpha^2}Var[(1+\alpha)^{X_n}]
$$


先求平方期望：
$$
\begin{align}
E[(1+\alpha)^{2X_n}]&=\sum_{j=0}^\infty P(X_n=j)E[(1+\alpha)^{2X_{n+1}}|X_n=j] \\
&=\sum_{j=0}^\infty P(X_n=j)((1-\frac1{(1+\alpha)^j})\cdot (1+\alpha)^{2j} + \frac1{(1+\alpha)^j}\cdot(1+\alpha)^{2j+2} \\
&=\sum_{j=0}^\infty P(X_n=j)((1+\alpha)^{2j}+\alpha(\alpha+2)(1+\alpha)^j) \\
&=E[(1+\alpha)^{2X_n}]+\alpha(\alpha+2)E[(1+\alpha)^{X_n}] \\
&=E[(1+\alpha)^{2X_n}]+\alpha(\alpha+2)(\alpha n +1) \\
&=E[(1+\alpha)^{2X_0}]+\sum_{j=0}^{n-1}\alpha (\alpha+2)(\alpha j +1)\\
&=(\frac{\alpha^3}2+\alpha^2)n^2+(2\alpha-\frac{\alpha^3}2)n+1\\
\end{align}
$$
由此计算方差：
$$
\begin{align}
Var[\frac{(1+\alpha)^{X_n}}\alpha]&=\frac1{\alpha^2}Var[(1+\alpha)^{X_n}] \\
&= \frac1{\alpha^2}(E[(1+\alpha)^{2X_n}]-E[(1+\alpha)^{X_n}]^2)\\
&=\frac1{\alpha^2}((\frac{\alpha^3}2+\alpha^2)n^2+(2\alpha-\frac{\alpha^3}2)n+1-(\alpha^2n^2+2\alpha n +1))\\
&=\frac{\alpha}2n^2-\frac{\alpha}2n \\
&<\frac{\alpha}2n^2
\end{align}
$$

### Q2

算法描述：

> 1. 独立运行s次上述算法，得到n的所有估计值$\tilde n_1, \tilde n_1, \dots,~\tilde n_s$，其中$s\ge \frac\alpha{2\delta\epsilon^2}$
> 2. 输出$\tilde n = \frac1s\sum_{i=1}^s\tilde n_i, $

正确性：
$$
E[\tilde n]=\frac1s\cdot s\cdot n=n \\
Var[\tilde n] = \frac1{s^2}\cdot s\cdot\frac\alpha2(n^2-n)\le \frac\alpha{2s}n^2
$$
由切比雪夫不等式可得：
$$
P[|\tilde n - n|>\epsilon n] < \frac{Var[\tilde n]}{\epsilon^2n^2}=\frac{\alpha}{2s\epsilon^2} \le \delta
$$
由此可知，该算法以至少$1-\delta$的概率，返回一个$n$的估计值$\tilde n$，满足$|\tilde n - n| \le \epsilon n$

最坏空间：
$$
s\log_2\log_{1+\alpha} n = O(\frac1{\delta\epsilon^2}\log\log n)
$$



## EX6

证明：设$Ham(\bold x,\bold y) = H$
$$
\begin {align}
\Pr[(U\bold x)_i \ne (U\bold y)_i] &= \Pr[\sum_{j=1}^du_{ij}(x_j-y_j) \ne0]\\
&=Pr[\sum_{x_j \ne y_j}u_{ij}(x_j-y_j)]
\end {align}
$$
注意到，这个求和式子一共有$H$项，在$mod2$意义下，$x_j-y_j = 1$，因此，我们只需要在这$H$个$u_{ij}$中选奇数个为1，其余为0即可，这显然是一个二项分布：
$$
\begin{align}
\Pr[(U\bold x)_i \ne (U\bold y)_i] &=\Pr[\sum_{x_j \ne y_j}u_{ij}(x_j-y_j)] \\
&= \sum _{k \in odd}C_H^kp^k(1-p)^{H-k} 
\end{align}
$$
下面我们就来求解这个组合求和式子，基于如下两个方程：
$$
\sum _{k \in odd}C_H^kp^k(1-p)^{H-k} + \sum _{k \in even}C_H^kp^k(1-p)^{H-k} = 1 \\
\sum _{k \in odd}C_H^k(-p)^k(1-p)^{H-k} + \sum _{k \in even}C_H^k(-p)^k(1-p)^{H-k} = (1-2p)^H
$$
由此推出：
$$
\Pr[(U\bold x)_i \ne (U\bold y)_i] = \sum _{k \in odd}C_H^kp^k(1-p)^{H-k} = \frac12(1-(1-2p)^{Ham{(\bold x, \bold y)}})
$$
