# HW 04

PB20111623 马子睿

## EX1

> 在CountSketch算法及其分析中，我们证明了如果选择$w>3k^2$，$d=\Omega(\log n)$， 那么以$1-\frac{1}{n}$的概率，对于任意$i\in [n]$，$\abs{\tilde{x}_i-x_i}\leq \frac{\norm{x}_2}{k}$。这个估计有可能在某些情况是比较坏的，例如当$\norm{x}_2$的值主要集中在少数几个坐标上的时候。
> 对于固定的整数$\ell>0$，对于任意$i\in [n]$，定义向量$y^{(i)}\in \mathbb{R}^n$如下：
> $$
> \begin{equation*}
> 	y^{(i)}_j=\begin{cases}
> 		0 & \text{如果$j=i$或者$j$是$x$中（在绝对值意义下）最大的$\ell$个值所对应的坐标之一},\\
> 		x_j & \text{否则}
> 	\end{cases}
> \end{equation*}
> $$
>
> 证明对于$\ell=k^2$，如果$w=6k^2$，$d=\Omega(\log n)$，那么以$1-\frac{1}{n}$的概率，对于任意$i\in [n]$，$\abs{\tilde{x}_i-x_i}\leq \frac{\norm{y^{(i)}}_2}{k}$。

借鉴CountSketch算法的证明思路：

固定一组$p \in [d]$和$i \in [n]$，设$Z_p = g_p(i)C(p, h_p(i))$，依然定义一个冲突指示变量$Y_j=I(h_p(i) = h_p(j))$

定义“大值”集合

$L_i = \{j|j = i或者j是x中(在绝对值意义下)最大的l个值所对应的坐标之一\}$

那么自然有：
$$
\begin{align}
Z_p &= g_p(i)(g_p(i)x_i+\sum_{j \ne i}Y_jg_p(j)x_j) \\
&= x_i+\sum_{j \ne i}Y_jg_p(i)g_p(j)x_j \\
&= x_i + \sum_{j \in L_i \backslash\{i\}} Y_jg_p(i)g_p(j)x_j + \sum_{j \in [n] \backslash L_i} Y_jg_p(i)g_p(j)x_j
\end{align}
$$

如此一来，我们就把$Z_p$中偏离$x_i$的项分为了两部分：一部分从属于上面定义的“大值”集合，而另一部分并不在这个集合中，下面我们只需要对这两个部分分别讨论即可。

令事件$M = \{\sum_{j \in L_i \backslash\{i\}} Y_jg_p(i)g_p(j)x_j = 0 \}$
$$
\begin{align}
\Pr(M) &\ge (1-\frac1w)^l \\
&\ge 1-\frac lw \\
&= \frac56
\end{align}
$$
 其中第一个不等式是因为，如果M成立，那么对于$\forall j \in L_i\backslash\{i\}$，有$Y_j = 0$，再根据二元独立哈希函数的性质可得。而第二个不等式是伯努利不等式的一般形式。

我们已经证明了有很大概率$\sum_{j \in L_i \backslash\{i\}} Y_jg_p(i)g_p(j)x_j = 0$，那么我们再设：

$U_p =  \sum_{j \in [n] \backslash L_i} Y_jg_p(i)g_p(j)x_j$

下面计算它的期望和方差，由独立性可得：
$$
E[U_p] = \sum_{j \in [n] \backslash L_i} E[Y_j]E[g_p(i)]E[g_p(j)]E[x_j] = 0
$$

$$
\begin{align}
Var[U_p] &= E[U_p^2] \\
&=E[\sum_{j \in [n] \backslash L_i}\sum_{j' \in [n] \backslash L_i}Y_jY_{j'}g_p(i)^2g_p(j)g_p(j')x_jx_{j'}] \\
&= \sum_{j \in [n] \backslash L_i}E[Y_j^2]x_j^2 \\
& \le \frac{||y^{(i)}||^2}w
\end{align}
$$

结合切比雪夫不等式可得：
$$
\begin{align}
\Pr(|U_p| \le \frac{||y^{(i)}||}w) 
&> 1- k^2\frac{Var[U_p]}{||y^{(i)}||^2}\\
&\ge 1- \frac{k^2}w \\
&= \frac56
\end{align}
$$
不妨设事件$N = \{|U_p| \le \frac{||y^{(i)}||}w \}$

当M和N同时发生，那样我们就满足了题目的条件，下面我们求一下不满足题目条件的概率
$$
\begin{align}
\Pr(|Z_p-x_i| > \frac{||y^{(i)}||}k) &\le \frac16 + \frac16 = \frac13 
\end{align}
$$
下面就是利用中位数的证明思路完成证明：

如果中位数估计也不满足条件，那么证明至少有一半估计失败了，现在设$Q_i =I(第i个估计成功了)$，那么显然：
$$
E[Q_i] = 1 \cdot \Pr[Q_i=1] + 0 \cdot \Pr[Q_i=0] > \frac23
$$
设$Q=\sum_{i=1}^dQ_i$，则$E[Q] = \frac23d$，根据 Chernoff Bound：
$$
\begin{align}
\Pr(\abs{\tilde{x}_i-x_i} \leq \frac{\norm{y^{(i)}}_2}{k}) &\le \Pr(Q \le \frac d2)\\
&< \Pr(Y-E[Y]<-\frac d6) \\
&\le \exp(-\frac{d}{18})
\end{align}
$$
只需要$d \ge 18 ln\frac1\delta$，算法就成功，而题目中给出了$d = O(logn)$，满足条件，得证。

## EX2

> 假设$k_1,k_2$是两个核（kernel）函数。证明：
> (a) 对于任意常数$c\geq 0$，$c k_1$是一个核函数。
> (b) 对于任意标量(scalar)函数 $f$，$k_3(x,y)=f(x)f(y)\cdot k_1(x,y)$是一个核函数。
> (c) $k_1+k_2$是一个核函数。
> (d) $k_1\cdot k_2$是一个核函数。

### (a) 

设$k_1(x_i,x_j)=\phi_1(x_i)^T\phi_1(x_j)$，可得
$$
\begin{align}
ck_1 &= c\phi_1(x_i)^T\phi_1(x_j) \\
&=(\sqrt c \phi_1(x_i))^T(\sqrt c \phi_1(x_j))
\end{align}
$$
这里只需令$\phi(x)=\sqrt c \phi_1(x)$，便可根据定义得知$ck_1$也是一个合法的核函数

### (b)

设$k_1(x_i,x_j)=\phi_1(x_i)^T\phi_1(x_j)$，可得
$$
\begin{align}
k_3(x_i, x_j) &= f(x_i)\phi_1(x_i)^Tf(x_j)\phi_1(x_j) \\
			  &=(f(x_i)\phi_1(x_i))^T(f(x_j)\phi_1(x_j)) \\
			  &= \phi_3(x_i)^T\phi_3(x_i)
\end{align}
$$
其中$\phi_3(x)=f(x)\phi_1(x)$，由核函数定义可知$k_3$也为合法核函数

### (c)

设$k_1$和$k_2$的核矩阵分别为$K_1$和$K_2$，再令$K=K_1+K_2$，显然$K$是$k_1+k_2$的核矩阵，由$K_1$和$K_2$的半正定可知：对于任意向量$\alpha$
$$
\begin{align}
\alpha^TK\alpha &= \alpha^T(K_1+K_2)\alpha \\
&=\alpha^TK_1\alpha+\alpha^TK_2\alpha \\
&\ge 0
\end{align}
$$


因此$K$仍半正定，故可知$k_1+k_2$仍为合法核函数

### (d)

设$k_1(x_i,x_j)=\phi_1(x_i)^T\phi_1(x_j)$，$k_2(x_i,x_j)=\phi_2(x_i)^T\phi_2(x_j)$

令$k(x, y) = k_1(x,y)\cdot k_2(x,y)$
$$
\begin{align}
k(x,y) 
&= \phi_1(x)^T\phi_1(y)\cdot \phi_2(x)^T\phi_2(y) \\
&= \sum_{i=1}^d\sum_{j=1}^d(\phi_{1i}(x)\phi_{2j}(x)\cdot \phi_{1i}(y)\phi_{2j}(y)) \\
&= \sum_{i=1}^d\sum_{j=1}^d\phi_{ij}(x)\cdot\phi_{ij}(y)
\end{align}
$$
其中$\phi_{ij}(x) = \phi_{1i}(x)\phi_{2j}(x)$，令$\phi_j(x)=(\phi_{1j}(x), \phi_{2j}(x),\dots, \phi_{dj}(x))$，再令$\phi(x) = (\phi_1(x),\phi_2(x),\dots ,\phi_d(x))$，那么这里显然有：
$$
\begin{align}
\phi(x)^T\phi(y) 
&=(\phi_1(x),\phi_2(x),\dots\phi_d(x))^T(\phi_1(y),\phi_2(y),\dots\phi_d(y))\\
&= \sum_{i=1}^d\phi_i(x)^T\phi_i(y) \\
&= \sum_{i=1}^d\sum_{j=1}^d\phi_{ij}(x)\cdot\phi_{ij}(y) \\
&= k(x,y)
\end{align}
$$
由核函数定义，$k_1 \cdot k_2$是合法的核函数

## EX3

> 考虑如下的在线学习（online learning）场景：在每个时刻$t=1,2,\dots,$, 下面两个事件依次发生：
> 		(a) 算法看到任意一个样本$x_t$，然后对它的标号（label）进行预测，令$\ell'_t$为其预测值
> 		(b) 接下来算法被告知样本的真实标号$\ell_t$，如果$\ell'_t\neq \ell_t$，则我们称其犯了一个错误
> 我们的目标是设计一个在线的分类算法，使得其犯错次数越小越好。
> 在课堂上，我们分析了对于线性可分的(linearly separable)数据样本集，感知机(Perceptron)算法的正确性与效率。现在我们考虑如下的感知机算法：
> 	1. 令$w=0$，即$w$为全$0$向量。对于时刻$t=1,2,\dots,$ 算法如下操作：
> 		(a) 对于样本$x_t$，预测其标号为$sgn(x_t^T w)$
> 		(b) 如果预测值是错误的，那么更新$w$为$w+x_t\ell_t$
> 	证明下面的结论：
>
> 对于任意的样本序列$x_1,x_2,\dots,$如果存在一个向量$w^*$满足对于任意的$t\geq 1$，$x_t^Tw^*\ell_t\ge 1$（即$(w^*)^Tx=0$是一个间隔(margin)至少为$\gamma=1/\norm{w^*}$的线性分割子），那么上述的感知机算法犯错的次数不超过$r^2\norm{w^*}^2$次，这里的$r=\max_t\norm{x_t}$。
>
> 提示：参考课堂上Perceptron算法的分析。

该算法分析类似于Perceptron算法的分析。我们追踪两个值：$w^Tw$和$|w|^2$

一旦算法的计算出现错误，则会让两个变量产生如下变化：

* $w^Tw$至少增加1
  $$
  \begin{align}
  (w+x_il_i)^Tw^* &= w^Tw^*+x_i^Tl_iw^* \\
  &\ge w^Tw^*+1
  \end{align}
  $$
  
* $|w|^2$至多增加$r^2$
  $$
  \begin{align}
  (w+x_il_i)^T(w+x_il_i) &= |w|^2+2x_i^Tl_iw+|x_il_i|^2 \\
  &\le |w|^2+|x_i|^2 \\
  &\le |w|^2+r^2
  \end{align}
  $$
  

  其中第一个不等式是因为，每一次更新只在$sgn(x_i^Tw)l_i \le 0$时刻发生

若一共出现了m次错误，那么显然有：
$$
|w^Tw|\ge m\\
|w| \le r\sqrt m
$$
因此得到：
$$
\begin{align}
m &\le|w||w^*| \\
m &\le r\sqrt m |w^*| \\
m &\le r^2|w^*|^2
\end{align}
$$
由此得证

## EX4

>考虑数据样本集合不一定线性可分的情形。给定了数据样本（及其标号）集合$(x_i,\ell_i),1\leq i\leq n$及一个向量$w^*$，定义（样本$x_i$相对于$w^*$的）hinge损失为
>$L_{hinge}(w^*,x_i)=\max\{0,1-x_i^Tw^*\ell_i\}$,对任意的$i\le n$
>证明下面的结论：
>对于一个样本序列$S=x_1,x_2,\dots,$，上题中所描述的（在线）感知机算法所犯错误最多为
>$\min_{w^*}(r^2\norm{w^*}^2+2L_{hinge}(w^*,S))$,
>
>这里$r=\max_t\norm{x_t}$， $
>L_{hinge}(w^*,S)=\sum_{i=1}^n L_{hinge}(w^*,x_i)$。
>
>
>提示：参考Perceptron算法的分析。注意到，对于一个标号为正的样本，每次$w^Tw^*$增加$x_t^Tw^*$，后者至少为$1-L_{hinge}(w^*,x_t)$。对于标号为负的情形类似。

依然参考Perceptron算法的分析，追踪$w^Tw$和$|w|^2$

一旦算法的计算出现错误，则会让两个变量产生如下变化：

* $w^Tw$至少增加$x_i^Tw^*$
  $$
  \begin{align}
  (w+x_il_i)^Tw^* &= w^Tw^*+x_i^Tl_iw^* 
  \end{align}
  $$
  如果$x_i^Tl_iw^* \ge1 $，那么根据定义，$L_{hinge}(w^*,x_i)=0$，此时必然有$x_i^Tl_iw^* \ge L_{hinge}(w^*,x_i)$

  如果$x_i^Tl_iw^* < 1 $，那么根据定义，$L_{hinge}(w^*,x_i)=1-x_i^Tl_iw^*$，此时必然有$x_i^Tl_iw^* = 1- L_{hinge}(w^*,x_i)$

  综上所述，可得：
  $$
  \begin{align}
  (w+x_il_i)^Tw^* &= w^Tw^*+x_i^Tl_iw^* \\
  &\ge w^Tw^* + 1- L_{hinge}(w^*,x_i)
  \end{align}
  $$
  
* $|w|^2$至多增加$r^2$
  $$
  \begin{align}
  (w+x_il_i)^T(w+x_il_i) &= |w|^2+2x_i^Tl_iw+|x_il_i|^2 \\
  &\le |w|^2+|x_i|^2 \\
  &\le |w|^2+r^2
  \end{align}
  $$
  

  其中第一个不等式是因为，每一次更新只在$sgn(x_i^Tw)l_i \le 0$时刻发生

设算法一共犯错m次，且发生错误的时间是$i_1, i_2, \dots, i_m$
$$
\begin{align}
w^Tw^* &= m - \sum_{i = i_1}^{i_m}L_{hinge}(w^*, x_i) \\
&\ge m -L_{hinge}(w^*,S)
\end{align}
$$
另外和上题中一致，有$|w|^2 \le mr^2$

因此有：
$$
\begin{align}
m-L_{hinge}(w^*,S) &\le|w||w^*| \\
m-L_{hinge}(w^*,S) &\le r\sqrt m |w^*| \\
m^2 - 2mL_{hinge}(w^*,S)+ L_{hinge}(w^*,S)^2&\le r^2 m|w^*|^2
\end{align}
$$
这里产生了一个一元二次不等式，令$L=L_{hinge}(w^*,S)$经过计算：
$$
\Delta=(2L+r^2|w^*|^2)^2-4L^2\\
m \le \frac{2L+r^2|w^*|^2+\sqrt{(2L+r^2|w^*|^2)^2-4L^2}}{2} \\
m \le \frac12(2L+r^2|w^*|^2+\sqrt{r^4|w^*|^4+4Lr^2|w^*|^2}) \\
m  \le \frac12(2L+ r^2|w^*|^2 + r^2|w^*|^2\sqrt{1+\frac{4L}{r^2|w^*|^2}})\\
m  \le \frac12(2L+ r^2|w^*|^2 + r^2|w^*|^2+2L)\\
m \le r^2|w^*|^2 +2L_{hinge}(w^*,S)
$$
其中，倒数第二步是使用了伯努利不等式

上述过程对于任意的$w^*$都成立，因此可以得到：
$$
m \le \min_{w^*}(r^2\norm{w^*}^2+2L_{hinge}(w^*,S))
$$
得证

## EX5

> 令实例空间（instance space）$X=\{0,1\}^d$，并令$\mathcal{H}$为所有的3-合取范式公式（3-CNF formula）所构成的类。具体来说，考虑所有的由至多3个文字(literal)的析取（即OR）所构成的逻辑子句(clause)，$\mathcal{H}$是所有的可以被描述成这样的子句的合取（conjunction）形式的概念（concepts）构成的集合。例如，目标概念$c^*$可能为$(x_1 \lor \bar{x_2}\lor x_3)\land (x_2\lor x_4)\land (\bar{x}_1\lor x_3)\land (x_2\lor x_3\lor x_4)$。假设我们在PAC-learning的设定中：训练数据中的样本(examples)是根据某个分布$D$抽样出来的，它们是根据某个3-合取范式公式$c^*$来被标号的。
> (a) 给出样本个数m的一个下界，保证以至少$1-\delta$的概率，对于所有的与训练数据一致(consistent)的3-合取范式公式，其错误都不超过$\varepsilon$，这里的错误是相对应于分布$D$而言的。
> (b) 假设存在一个3-合取范式公式与训练数据一致，给出一个多项式时间的算法来找到一个这样的公式。

### (a)

在这个条件下，每一个基本变元可以是$x_i$或$\bar {x_i}$，因此我们一共有$2d$个选择，每次有放回的的选3个作为析取支，那么这样选择出的析取支一共有$(2d)^3$个

对于任何一个析取支而言，在$\mathcal{H}$中可以包含也可以不包含，因此$|\mathcal H|=2^{(2d)^3}$，根据教材中的定理5.4，我们可以给出一个$m$的下界：
$$
m \ge \frac1\epsilon((2d)^3\cdot ln2+ln\frac1\delta)
$$

### (b)

1. 初始化：设将所有$2d^3$个析取支都合取在一起，不妨设为$a_1, a_2, \dots,a_{(2d)^3}$，则这个3-合取范式为$h = a_1 \wedge a_2\wedge \cdots \wedge a_{(2d)^3}$
2. 对于每一个输入的训练集中的元组$(x^{(i)}, l_i)$:
   * 如果$l_i=0$，那么忽略掉这个元组
   * 如果$l_i=1$，那么搜索$h$中全部析取支，一旦$x^{(i)}$使得某一个析取支的取值为0，就从$h$中剪除这个析取支。也就是说，若$x^{(i)}$使得$a_j=0$，那么剪除后$h = a_1 \wedge \cdots \wedge a_{j-1}\wedge a_{j+1}\wedge \cdots\wedge a_{(2d)^3}$

3. 处理完训练集中全部元组后，输出$h$

显然，这个算法对训练集中每一个元组进行遍历，而对于每一个元组也需要对每一个析取支遍历，算法时间复杂度为$O(md^3)$，是多项式时间。
